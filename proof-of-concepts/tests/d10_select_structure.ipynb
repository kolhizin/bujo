{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_config as cfg\n",
    "import numpy as np\n",
    "import itertools, functools\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, os.path, time, datetime\n",
    "import pickle, io, json\n",
    "\n",
    "import skimage, skimage.io, skimage.transform, skimage.filters\n",
    "import sklearn, sklearn.metrics\n",
    "\n",
    "import importlib\n",
    "sys.path.append('../src/')\n",
    "import modutils\n",
    "import word_processing as wp\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "class SequentialTransformer:\n",
    "    def __init__(self, *args):\n",
    "        self.stages_ = args\n",
    "        \n",
    "    def transform(self, x):\n",
    "        res = x\n",
    "        for s in self.stages_:\n",
    "            res = s.transform(res)\n",
    "        return res\n",
    "    \n",
    "class LoadImageTransformer(BaseTransformer):\n",
    "    def __init__(self, path):\n",
    "        self.path_ = path\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if type(x) != str:\n",
    "            raise Exception(\"LoadImageTransformer: expects filename as argument!\")\n",
    "        return skimage.io.imread(os.path.join(self.path_, x), as_grey=True)\n",
    "    \n",
    "class ConvertFloatTransformer(BaseTransformer):\n",
    "    def __init__(self, min_value = 0.0, max_value = 1.0):\n",
    "        self.min_ = min_value\n",
    "        self.max_ = max_value\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if x.dtype in (np.float, np.float64, np.float32):\n",
    "            return x\n",
    "        if x.dtype == np.uint8:\n",
    "            return (x / 255.0) * (self.max_ - self.min_) + self.min_\n",
    "        if x.dtype == np.uint16:\n",
    "            return (x / 65535.0) * (self.max_ - self.min_) + self.min_\n",
    "        raise Exception(\"ConvertFloatTransformer: unexpected argument type!\")\n",
    "    \n",
    "class RandomStretchTransformer(BaseTransformer):\n",
    "    def __init__(self, min_scale = 0.66, max_scale = 1.5, fill_value=1.0):\n",
    "        self.max_ = max_scale\n",
    "        self.min_ = min_scale\n",
    "        self.fill_ = fill_value\n",
    "        \n",
    "    def transform(self, x):\n",
    "        f = np.random.uniform(self.min_, self.max_)\n",
    "        return skimage.transform.rescale(x, (1.0, f), mode='constant', cval=self.fill_)\n",
    "    \n",
    "class TransposeTransformer(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self, x):\n",
    "        return np.transpose(x)\n",
    "    \n",
    "class FitSizeTransformer(BaseTransformer):\n",
    "    def __init__(self, width, height, fill_value=1.0):\n",
    "        self.w_ = width\n",
    "        self.h_ = height\n",
    "        self.fill_ = fill_value\n",
    "        self.template_ = np.ones((self.h_, self.w_)) * self.fill_\n",
    "        \n",
    "    def transform(self, x):\n",
    "        (h, w) = x.shape\n",
    "        f = max(w / self.w_, h / self.h_)\n",
    "        res = self.template_.copy()\n",
    "        rw = max(min(self.w_, int(w / f)), 1)\n",
    "        rh = max(min(self.h_, int(h / f)), 1)\n",
    "        res[0:rh, 0:rw] = skimage.transform.resize(x, (rh, rw), mode='constant', cval=self.fill_)\n",
    "        return res\n",
    "    \n",
    "class StandardizeTransformer(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, x):\n",
    "        m = np.mean(x)\n",
    "        s = np.std(x)\n",
    "        if s <= 1e-9:\n",
    "            return x - m\n",
    "        return (x - m) / s\n",
    "    \n",
    "class TruncateLabelTransform(BaseTransformer):\n",
    "    def __init__(self, max_cost):\n",
    "        self.max_cost_ = max_cost\n",
    "        \n",
    "    def transform(self, x):\n",
    "        if type(x) != str:\n",
    "            raise Exception(\"TruncateLabelTransform: input expected to be of type string!\")\n",
    "        cost = 0\n",
    "        for i in range(len(x)):\n",
    "            flg = (i > 0) and (x[i] == x[i-1])\n",
    "            cost += 1 + int(flg)\n",
    "            if cost > max_cost:\n",
    "                return x[:i]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTRModel:\n",
    "    def __init__(self, charlist, img_size=(128, 32), text_len=32,\n",
    "                 cnn_kernels = [5, 5, 3, 3, 3],\n",
    "                 cnn_features = [1, 32, 64, 128, 128, 256],\n",
    "                 cnn_pools = [(2,2), (2,2), (1,2), (1,2), (1,2)],\n",
    "                 cnn_strides = None,\n",
    "                 rnn_cells = [256, 256],\n",
    "                 decoder = 'best-path',\n",
    "                 model_path = '/htr-model/',\n",
    "                 restore=False):\n",
    "        self.chars_ = charlist\n",
    "        self.restore_ = restore\n",
    "        self.epochID_ = 0\n",
    "        self.img_size_ = img_size\n",
    "        self.text_len_ = text_len\n",
    "        \n",
    "        self.cnn_kernels_ = cnn_kernels\n",
    "        self.cnn_features_ = cnn_features\n",
    "        self.cnn_pools_ = cnn_pools\n",
    "        self.cnn_strides_ = cnn_strides\n",
    "        if cnn_pools is None and cnn_strides is None:\n",
    "            raise Exception(\"Must specify at least one of `pools` and `strides`!\")\n",
    "        if cnn_pools is None:\n",
    "            self.cnn_pools_ = cnn_strides\n",
    "        if cnn_strides is None:\n",
    "            self.cnn_strides_ = cnn_pools\n",
    "        self.rnn_cells_ = rnn_cells\n",
    "        self.model_path_ = model_path\n",
    "        \n",
    "        if decoder not in ('best-path', 'beam-search'):\n",
    "            raise Exception(\"HTRModel: unknown decoder name `{}`. Expected `best-path` or `beam-search`\".format(decoder))\n",
    "        self.decoder_ = decoder\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.tf_is_train_ = tf.placeholder_with_default(False, shape=[], name='is_train')\n",
    "        self.tf_in_images_ = tf.placeholder(tf.float32, shape=(None, self.img_size_[0], self.img_size_[1]))\n",
    "\n",
    "        self.tf_cnn_out_ = HTRModel.setupCNN_(self.tf_in_images_, self.tf_is_train_,\n",
    "                    self.cnn_kernels_, self.cnn_features_, self.cnn_pools_, self.cnn_strides_)\n",
    "        \n",
    "        #self.tf_rnn_out_ = HTRModel.setupRNN_(self.tf_cnn_out_, len(self.chars_), self.rnn_cells_)\n",
    "        self.tf_rnn_out_ = HTRModel.setupStaticRNN_(self.tf_cnn_out_, len(self.chars_), self.rnn_cells_)\n",
    "        #self.setupCTC_()\n",
    "\n",
    "        self.snap_id_ = 0\n",
    "        self.trained_samples_ = 0\n",
    "        self.tf_learning_rate_ = tf.placeholder(tf.float32, shape=[])\n",
    "        #self.tf_update_ops_ = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n",
    "        #with tf.control_dependencies(self.tf_update_ops_):\n",
    "        #    self.tf_optimizer_ = tf.train.RMSPropOptimizer(self.tf_learning_rate_).minimize(self.tf_loss_)\n",
    "\n",
    "        (self.tf_session_, self.tf_saver_) = HTRModel.setupTF_(model_path)\n",
    "        \n",
    "    def setupCNN_(tf_input, tf_is_train, kernels, features, pools, strides):\n",
    "        chk1 = len(kernels)+1 != len(features)\n",
    "        chk2 = len(kernels) != len(pools)\n",
    "        chk3 = len(pools) != len(strides)\n",
    "        if chk1 or chk2 or chk3:\n",
    "            print(len(kernels), len(pools), len(strides), len(features))\n",
    "            raise Exception(\"HTRModel.setupCNN: lengths of arguments mismatch!\")\n",
    "            \n",
    "        tf_cnn_input = tf.expand_dims(input=tf_input, axis=3)\n",
    "\n",
    "        pool = tf_cnn_input\n",
    "        for i in range(len(kernels)):\n",
    "            kernel = tf.Variable(tf.truncated_normal([kernels[i], kernels[i], features[i], features[i + 1]], stddev=0.1))\n",
    "            conv = tf.nn.conv2d(pool, kernel, padding='SAME',  strides=(1,1,1,1))\n",
    "            conv_norm = conv #tf.layers.batch_normalization(conv, training=tf_is_train) #had to remove for tflite\n",
    "            relu = tf.nn.relu(conv_norm)\n",
    "            pool = tf.nn.max_pool(relu, (1, pools[i][0], pools[i][1], 1), (1, strides[i][0], strides[i][1], 1), 'VALID')\n",
    "\n",
    "        return pool\n",
    "\n",
    "\n",
    "    def setupRNN_(tf_input, charnum, cell_sizes):\n",
    "        rnn_input = tf.squeeze(tf_input, axis=[2])\n",
    "        #default [256, 256]\n",
    "\n",
    "        cells = [tf.contrib.rnn.LSTMCell(num_units=x, state_is_tuple=True) for x in cell_sizes]\n",
    "        stacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # bidirectional RNN, BxTxF -> BxTx2H\n",
    "        ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_input, dtype=rnn_input.dtype)\n",
    "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "        \n",
    "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "        kernel = tf.Variable(tf.truncated_normal([1, 1, sum(cell_sizes), charnum + 1], stddev=0.1))\n",
    "        return tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
    "    \n",
    "    \n",
    "    def setupStaticRNN_(tf_input, charnum, cell_sizes):\n",
    "        rnn_input = tf.squeeze(tf_input, axis=[2])\n",
    "\n",
    "        cells = [tf.contrib.rnn.LSTMCell(num_units=x, state_is_tuple=True) for x in cell_sizes]\n",
    "        stacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "        \n",
    "        (res, _) = tf.nn.dynamic_rnn(cell=stacked, inputs=rnn_input, dtype=rnn_input.dtype)\n",
    "        return res\n",
    "\n",
    "        # bidirectional RNN, BxTxF -> BxTx2H\n",
    "        ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_input, dtype=rnn_input.dtype)\n",
    "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "        \n",
    "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "        kernel = tf.Variable(tf.truncated_normal([1, 1, sum(cell_sizes), charnum + 1], stddev=0.1))\n",
    "        return tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
    "    \n",
    "    def setupCTC_(self):\n",
    "        self.tf_ctc_in_ = tf.transpose(self.tf_rnn_out_, [1, 0, 2]) # BxTxC -> TxBxC\n",
    "        # ground truth text as sparse tensor\n",
    "        self.tf_ctc_gt_ = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]) , tf.placeholder(tf.int32, [None]), tf.placeholder(tf.int64, [2]))\n",
    "\n",
    "        # calc loss for batch\n",
    "        self.tf_seq_len_ = tf.placeholder(tf.int32, [None])\n",
    "        self.tf_loss_ = tf.reduce_mean(tf.nn.ctc_loss(labels=self.tf_ctc_gt_, inputs=self.tf_ctc_in_, sequence_length=self.tf_seq_len_, ctc_merge_repeated=True))\n",
    "\n",
    "        # calc loss for each element to compute label probability\n",
    "        self.tf_ctc_in_saved_ = tf.placeholder(tf.float32, shape=[self.text_len_, None, len(self.chars_) + 1])\n",
    "        self.tf_loss_per_elem_ = tf.nn.ctc_loss(labels=self.tf_ctc_gt_, inputs=self.tf_ctc_in_saved_, sequence_length=self.tf_seq_len_, ctc_merge_repeated=True)\n",
    "\n",
    "        if self.decoder_ == 'best-path':\n",
    "            self.tf_decoder_ = tf.nn.ctc_greedy_decoder(inputs=self.tf_ctc_in_, sequence_length=self.tf_seq_len_)\n",
    "        elif self.decoder_ == 'beam-search':\n",
    "            self.tf_decoder_ = tf.nn.ctc_beam_search_decoder(inputs=self.tf_ctc_in_, sequence_length=self.tf_seq_len_, beam_width=50, merge_repeated=False)\n",
    "    \n",
    "    def setupTF_(model_path, max_to_keep=1):\n",
    "        print('Python: {}; TF: {}'.format(sys.version, tf.__version__))\n",
    "        sess=tf.Session()\n",
    "        saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "        latest_snapshot = tf.train.latest_checkpoint(model_path)\n",
    "        if latest_snapshot:\n",
    "            print('Starting hot: {}'.format(latest_snapshot))\n",
    "            saver.restore(sess, latest_snapshot)\n",
    "        else:\n",
    "            print('Starting cold')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        return (sess,saver)\n",
    "    \n",
    "    \n",
    "    def encodeLabels(self, texts):\n",
    "        indices = []\n",
    "        values = []\n",
    "        shape = [len(texts), max(len(x) for x in texts)] # last entry must be max(labelList[i])\n",
    "\n",
    "        for (i, text) in enumerate(texts):\n",
    "            encoded_text = [self.chars_.index(c) for c in text]\n",
    "            for (j, label) in enumerate(encoded_text):\n",
    "                indices.append([i, j])\n",
    "                values.append(label)\n",
    "        return (indices, values, shape)\n",
    "\n",
    "\n",
    "    def decodeOutput(self, ctc_output, batch_size):\n",
    "        encoded_labels = [[] for i in range(batch_size)]\n",
    "\n",
    "        decoded=ctc_output[0][0] \n",
    "        # go over all indices and save mapping: batch -> values\n",
    "        for (k, (i, j)) in enumerate(decoded.indices):\n",
    "            label = decoded.values[k]\n",
    "            encoded_labels[i].append(label)\n",
    "\n",
    "        return [''.join([self.chars_[c] for c in x]) for x in encoded_labels]\n",
    "    \n",
    "    def getLearningRate(self):\n",
    "        return 0.01 if self.trained_samples_ < 1e4 else (0.001 if self.trained_samples_ < 1e5 else 0.0001)\n",
    "\n",
    "\n",
    "    def trainBatch(self, imgs, texts):\n",
    "        batch_size = len(imgs)\n",
    "        gt_sparse = self.encodeLabels(texts)\n",
    "        rate =  self.getLearningRate()\n",
    "        evalList = [self.tf_optimizer_, self.tf_loss_]\n",
    "        feedDict = {self.tf_in_images_ : imgs,\n",
    "                    self.tf_ctc_gt_ : gt_sparse,\n",
    "                    self.tf_seq_len_ : [self.text_len_] * batch_size,\n",
    "                    self.tf_learning_rate_ : rate,\n",
    "                    self.tf_is_train_: True}\n",
    "        (_, lossVal) = self.tf_session_.run(evalList, feedDict)\n",
    "        self.trained_samples_ += batch_size\n",
    "        return lossVal\n",
    "    \n",
    "    def validBatch(self, imgs, texts):\n",
    "        batch_size = len(imgs)\n",
    "        gt_sparse = self.encodeLabels(texts)\n",
    "        evalList = [self.tf_decoder_, self.tf_loss_]\n",
    "        feedDict = {self.tf_in_images_ : imgs,\n",
    "                    self.tf_ctc_gt_ : gt_sparse,\n",
    "                    self.tf_seq_len_ : [self.text_len_] * batch_size,\n",
    "                    self.tf_is_train_: False}\n",
    "        (evalRes, lossVal) = self.tf_session_.run(evalList, feedDict)\n",
    "        return self.decodeOutput(evalRes, batch_size), lossVal\n",
    "    \n",
    "    def inferBatch(self, imgs):\n",
    "        batch_size = len(imgs)\n",
    "        evalList = [self.tf_decoder_]\n",
    "        feedDict = {self.tf_in_images_ : imgs,\n",
    "                    self.tf_seq_len_ : [self.text_len_] * batch_size,\n",
    "                    self.tf_is_train_: False}\n",
    "        evalRes = self.tf_session_.run(evalList, feedDict)\n",
    "        return self.decodeOutput(evalRes[0], batch_size)\n",
    "\n",
    "    def save(self):\n",
    "        self.snap_id_ += 1\n",
    "        self.tf_saver_.save(self.tf_session_, self.model_path_ + 'snapshot', global_step=self.snap_id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timedelta(seconds):\n",
    "    if seconds < 1e-10:\n",
    "        return '0s'\n",
    "    sf = seconds - np.floor(seconds)\n",
    "    si = int(np.floor(seconds))\n",
    "    d, s_h = divmod(si, 3600*24)\n",
    "    h, s_m = divmod(s_h, 3600)\n",
    "    m, s = divmod(s_m, 60)\n",
    "    if d > 9:\n",
    "        return '{}d'.format(d)\n",
    "    elif d > 0:\n",
    "        return '{}d {}h'.format(d, h)\n",
    "    elif h > 9:\n",
    "        return '{}h'.format(h)\n",
    "    elif h > 0:\n",
    "        return '{}h {}m'.format(h, m)\n",
    "    elif m > 9:\n",
    "        return '{}m'.format(m)\n",
    "    elif m > 0:\n",
    "        return '{}m {}s'.format(m, s)\n",
    "    elif s > 9:\n",
    "        return '{}s'.format(s)\n",
    "    elif s > 0:\n",
    "        return '{:.1f}s'.format(s + sf)\n",
    "    else:\n",
    "        return '{}ms'.format(int(sf*1000))\n",
    "    \n",
    "def apply_esmooth(array, factor):\n",
    "    tmp = np.exp(np.cumsum([factor]*len(array)))\n",
    "    tmp = tmp / np.sum(tmp)\n",
    "    return np.sum(tmp * array)\n",
    "    \n",
    "def train(model, imgs, labels, batch_size, transform_pipeline=BaseTransformer()):\n",
    "    num = len(imgs)\n",
    "    num_batches = num // batch_size\n",
    "    ids = np.arange(num)\n",
    "    np.random.shuffle(ids)\n",
    "    text_template = 'Train batch {}/{}. Loss: {:.2f}. Time: {}. ETA: {}.'\n",
    "    hist_times = []\n",
    "    t_start = time.perf_counter()\n",
    "    sum_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        t0 = time.perf_counter()\n",
    "        batch_ids = ids[(i*batch_size):((i+1)*batch_size)]\n",
    "        batch_imgs = np.array([transform_pipeline.transform(imgs[j]) for j in batch_ids])\n",
    "        batch_lbls = np.array([labels[j] for j in batch_ids])\n",
    "        loss = model.trainBatch(batch_imgs, batch_lbls)\n",
    "        sum_loss += loss * len(batch_lbls)\n",
    "        t1 = time.perf_counter()\n",
    "        hist_times.append(t1-t0)\n",
    "        t_delta = apply_esmooth(np.array(hist_times)[::-1], -0.5)\n",
    "        t_eta = t_delta * (num_batches - i - 1)\n",
    "        print(text_template.format(i+1, num_batches, loss, format_timedelta(t1-t0), format_timedelta(t_eta)))\n",
    "    return sum_loss / len(labels)\n",
    "        \n",
    "def validate(model, imgs, labels, batch_size,\n",
    "             transform_pipeline=BaseTransformer()):\n",
    "    n_char_err = 0\n",
    "    n_char = 0\n",
    "    n_word_ok = 0\n",
    "    n_word = 0\n",
    "    num_batches = len(imgs) // batch_size\n",
    "    text_template = 'Validation batch {}/{}. Time: {}. ETA: {}.'\n",
    "    hist_times = []\n",
    "    t_start = time.perf_counter()\n",
    "    sum_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        t0 = time.perf_counter()\n",
    "        batch_imgs = np.array([transform_pipeline.transform(x)\n",
    "                               for x in imgs[(i*batch_size):((i+1)*batch_size)]])\n",
    "        batch_lbls = np.array(labels[(i*batch_size):((i+1)*batch_size)])\n",
    "        recognized, loss = model.validBatch(batch_imgs, batch_lbls)\n",
    "        sum_loss += loss * len(batch_lbls)\n",
    "        for j in range(len(recognized)):\n",
    "            n_word_ok += int(batch_lbls[j] == recognized[j])\n",
    "            n_word += 1\n",
    "            dist = editdistance.eval(recognized[j], batch_lbls[j])\n",
    "            n_char_err += dist\n",
    "            n_char += len(batch_lbls[j])\n",
    "        t1 = time.perf_counter()\n",
    "        hist_times.append(t1-t0)\n",
    "        t_delta = apply_esmooth(np.array(hist_times)[::-1], -0.5)\n",
    "        t_eta = t_delta * (num_batches - i - 1)\n",
    "        t_eta = (t1 - t_start) / (i + 1) * (num_batches - i - 1)\n",
    "        print(text_template.format(i+1, num_batches, format_timedelta(t1-t0), format_timedelta(t_eta)))\n",
    "\n",
    "    cer = n_char_err / n_char\n",
    "    wa = n_word_ok / n_word\n",
    "    print('Validation results: CER: {:.3f}, WA: {:.3f}.'.format(cer, wa))\n",
    "    return sum_loss/len(labels), cer, wa\n",
    "\n",
    "\n",
    "def run_training(model, train_imgs, train_labels, valid_imgs, valid_labels,\n",
    "                 batch_size=128, transform_pipeline=BaseTransformer()):\n",
    "    epoch = 0\n",
    "    text_template = 'Epoch {} complete in {}. T-loss is {:.2f}, V-loss is {:.2f}'\n",
    "    while True:\n",
    "        t0 = time.perf_counter()\n",
    "        epoch += 1\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "\n",
    "        tloss = train(model, train_imgs, train_labels, batch_size, transform_pipeline=transform_pipeline)\n",
    "        vloss, cer, wa = validate(model, valid_imgs, valid_labels, batch_size, transform_pipeline=transform_pipeline)\n",
    "        model.save()\n",
    "        t1 = time.perf_counter()\n",
    "        print(text_template.format(epoch, format_timedelta(t1-t0), tloss, vloss))\n",
    "        \n",
    "def load_sample(fname):\n",
    "    path = '.'.join(fname.split('.')[:-1])\n",
    "    sample = [tuple(y.strip() for y in x.split(' ')) for x in open(fname, 'r').readlines()]\n",
    "    load_pipeline = SequentialTransformer(LoadImageTransformer(path), ConvertFloatTransformer())\n",
    "    return [x for x in sample if len(x[1])>0], load_pipeline\n",
    "\n",
    "        \n",
    "def prepare_sample(sample, pipeline):\n",
    "    imgs = [pipeline.transform('{}.png'.format(x)) for (x,_) in sample]\n",
    "    lbls = [x for (_,x) in sample]\n",
    "    return imgs, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:49: UserWarning: `as_grey` has been deprecated in favor of `as_gray`\n",
      "  warn('`as_grey` has been deprecated in favor of `as_gray`')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sample, train_load_pipeline = load_sample('D:/Data/HTR/train.txt')\n",
    "train_imgs, train_lbls = prepare_sample(train_sample, train_load_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlist = sorted(list(functools.reduce(set.union, [set(x) for x in train_lbls])))\n",
    "transform_pipeline = SequentialTransformer(\n",
    "    RandomStretchTransformer(),\n",
    "    FitSizeTransformer(128, 32),\n",
    "    TransposeTransformer(),\n",
    "    StandardizeTransformer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]; TF: 1.14.0\n",
      "Starting cold\n",
      "Wall time: 406 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = HTRModel(charlist, img_size=(128, 32),\n",
    "                cnn_kernels = [5, 5, 3, 3], #[5, 5, 3, 3, 3],#default\n",
    "                cnn_features = [1, 32, 64, 64, 128], #[1, 32, 64, 128, 128, 256],#default\n",
    "                cnn_pools = [(2,2), (2,2), (1,2), (1,4)], #[(2,2), (2,2), (1,2), (1,2), (1,2)] #default\n",
    "                rnn_cells = [128, 128], #default \n",
    "                model_path='/htr-model-tmp/'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(?, 32, 128) dtype=float32>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tf_rnn_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_converter = tf.lite.TFLiteConverter.from_session(model.tf_session_,\n",
    "                                     [model.tf_in_images_],\n",
    "                                     [model.tf_rnn_out_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConverterError",
     "evalue": "TOCO failed. See console for info.\n2019-07-19 21:24:28.119519: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.120787: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"CPU\"') for unknown op: NoOp\n2019-07-19 21:24:28.121226: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"GPU\"') for unknown op: NoOp\n2019-07-19 21:24:28.121651: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostRecv\n2019-07-19 21:24:28.122162: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"CPU\"') for unknown op: _Send\n2019-07-19 21:24:28.122658: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"CPU\"') for unknown op: _HostRecv\n2019-07-19 21:24:28.123261: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"GPU\"') for unknown op: _Send\n2019-07-19 21:24:28.123860: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"CPU\"') for unknown op: _Recv\n2019-07-19 21:24:28.124458: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostSend\n2019-07-19 21:24:28.125161: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"GPU\"') for unknown op: _Recv\n2019-07-19 21:24:28.125794: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"CPU\"') for unknown op: _HostSend\n2019-07-19 21:24:28.126365: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\n2019-07-19 21:24:28.127089: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\n2019-07-19 21:24:28.127670: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\n2019-07-19 21:24:28.128310: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\n2019-07-19 21:24:28.129391: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.129883: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.130433: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.131004: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.131498: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.132097: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.132621: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.133137: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.133645: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.134181: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.134697: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.135227: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\n2019-07-19 21:24:28.135761: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayV3\n2019-07-19 21:24:28.136465: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\n2019-07-19 21:24:28.136990: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayV3\n2019-07-19 21:24:28.137401: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.137730: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.138088: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.138583: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.139100: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.139618: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.140122: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.140602: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.141122: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3\n2019-07-19 21:24:28.141604: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayScatterV3\n2019-07-19 21:24:28.142344: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.142861: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.143387: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.143982: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.144549: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.145075: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.145672: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.146099: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.146458: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.146807: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.147506: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.148045: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.148600: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond\n2019-07-19 21:24:28.149099: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: LoopCond\n2019-07-19 21:24:28.149680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit\n2019-07-19 21:24:28.150221: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Exit\n2019-07-19 21:24:28.150752: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3\n2019-07-19 21:24:28.151299: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayReadV3\n2019-07-19 21:24:28.151860: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\n2019-07-19 21:24:28.152360: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArraySizeV3\n2019-07-19 21:24:28.152922: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\n2019-07-19 21:24:28.153451: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayGatherV3\n2019-07-19 21:24:28.154259: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\n2019-07-19 21:24:28.154668: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayWriteV3\n2019-07-19 21:24:28.158450: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 133 operators, 215 arrays (0 quantized)\n2019-07-19 21:24:28.162349: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 133 operators, 215 arrays (0 quantized)\n2019-07-19 21:24:28.169661: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 87 operators, 145 arrays (0 quantized)\n2019-07-19 21:24:28.173509: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 86 operators, 143 arrays (0 quantized)\n2019-07-19 21:24:28.178748: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 86 operators, 143 arrays (0 quantized)\n2019-07-19 21:24:28.182498: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 86 operators, 143 arrays (0 quantized)\n2019-07-19 21:24:28.186596: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 672192 bytes, theoretical optimal value: 655808 bytes.\n2019-07-19 21:24:28.188553: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RANGE, RESHAPE, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\nTraceback (most recent call last):\r\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RANGE, RESHAPE, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    896\u001b[0m           \u001b[0minput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m           \u001b[0moutput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m           **converter_kwargs)\n\u001b[0m\u001b[0;32m    899\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m       result = _toco_convert_graph_def(\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[1;34m(input_data, input_tensors, output_tensors, *args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m   data = toco_convert_protos(model_flags.SerializeToString(),\n\u001b[0;32m    403\u001b[0m                              \u001b[0mtoco_flags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                              input_data.SerializeToString())\n\u001b[0m\u001b[0;32m    405\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[1;34m(model_flags_str, toco_flags_str, input_data_str)\u001b[0m\n\u001b[0;32m    170\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m       raise ConverterError(\n\u001b[1;32m--> 172\u001b[1;33m           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\n\u001b[0m\u001b[0;32m    173\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;31m# Must manually cleanup files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConverterError\u001b[0m: TOCO failed. See console for info.\n2019-07-19 21:24:28.119519: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.120787: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"CPU\"') for unknown op: NoOp\n2019-07-19 21:24:28.121226: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"GPU\"') for unknown op: NoOp\n2019-07-19 21:24:28.121651: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostRecv\n2019-07-19 21:24:28.122162: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"CPU\"') for unknown op: _Send\n2019-07-19 21:24:28.122658: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"CPU\"') for unknown op: _HostRecv\n2019-07-19 21:24:28.123261: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"GPU\"') for unknown op: _Send\n2019-07-19 21:24:28.123860: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"CPU\"') for unknown op: _Recv\n2019-07-19 21:24:28.124458: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostSend\n2019-07-19 21:24:28.125161: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"GPU\"') for unknown op: _Recv\n2019-07-19 21:24:28.125794: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"CPU\"') for unknown op: _HostSend\n2019-07-19 21:24:28.126365: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\n2019-07-19 21:24:28.127089: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\n2019-07-19 21:24:28.127670: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\n2019-07-19 21:24:28.128310: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\n2019-07-19 21:24:28.129391: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.129883: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.130433: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.131004: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.131498: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.132097: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.132621: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.133137: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.133645: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.134181: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.134697: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.135227: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\n2019-07-19 21:24:28.135761: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayV3\n2019-07-19 21:24:28.136465: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\n2019-07-19 21:24:28.136990: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayV3\n2019-07-19 21:24:28.137401: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.137730: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.138088: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.138583: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.139100: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.139618: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.140122: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.140602: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.141122: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3\n2019-07-19 21:24:28.141604: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayScatterV3\n2019-07-19 21:24:28.142344: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.142861: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.143387: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.143982: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.144549: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.145075: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.145672: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.146099: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.146458: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.146807: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.147506: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\n2019-07-19 21:24:28.148045: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Enter\n2019-07-19 21:24:28.148600: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond\n2019-07-19 21:24:28.149099: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: LoopCond\n2019-07-19 21:24:28.149680: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit\n2019-07-19 21:24:28.150221: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Exit\n2019-07-19 21:24:28.150752: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3\n2019-07-19 21:24:28.151299: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayReadV3\n2019-07-19 21:24:28.151860: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\n2019-07-19 21:24:28.152360: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArraySizeV3\n2019-07-19 21:24:28.152922: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\n2019-07-19 21:24:28.153451: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayGatherV3\n2019-07-19 21:24:28.154259: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\n2019-07-19 21:24:28.154668: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: TensorArrayWriteV3\n2019-07-19 21:24:28.158450: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 133 operators, 215 arrays (0 quantized)\n2019-07-19 21:24:28.162349: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 133 operators, 215 arrays (0 quantized)\n2019-07-19 21:24:28.169661: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 87 operators, 145 arrays (0 quantized)\n2019-07-19 21:24:28.173509: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 86 operators, 143 arrays (0 quantized)\n2019-07-19 21:24:28.178748: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 86 operators, 143 arrays (0 quantized)\n2019-07-19 21:24:28.182498: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 86 operators, 143 arrays (0 quantized)\n2019-07-19 21:24:28.186596: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 672192 bytes, theoretical optimal value: 655808 bytes.\n2019-07-19 21:24:28.188553: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RANGE, RESHAPE, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\nTraceback (most recent call last):\r\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RANGE, RESHAPE, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\n\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tflite_model = tflite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch normalization does not work in TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "static_rnn() missing 2 required positional arguments: 'cell' and 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-a97075f9e720>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: static_rnn() missing 2 required positional arguments: 'cell' and 'inputs'"
     ]
    }
   ],
   "source": [
    "tf.nn.static_rnn(tf.nn.LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
